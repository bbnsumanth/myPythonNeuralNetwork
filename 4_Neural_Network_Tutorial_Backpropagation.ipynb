{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-requisites\n",
    "import numpy as np\n",
    "\n",
    "# To clear print buffer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing code from the previous tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing weight matrices from layer sizes\n",
    "def initializeWeights(layers):\n",
    "    weights = [np.random.randn(o, i+1) for i, o in zip(layers[:-1], layers[1:])]\n",
    "    return weights\n",
    "\n",
    "# Add a bias term to every data point in the input\n",
    "def addBiasTerms(X):\n",
    "        # Make the input an np.array()\n",
    "        X = np.array(X)\n",
    "        \n",
    "        # Forcing 1D vectors to be 2D matrices of 1xlength dimensions\n",
    "        if X.ndim==1:\n",
    "            X = np.reshape(X, (1, len(X)))\n",
    "        \n",
    "        # Inserting bias terms\n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# Sigmoid function\n",
    "def sigmoid(a):\n",
    "    return 1/(1 + np.exp(-a))\n",
    "\n",
    "# Forward Propagation of outputs\n",
    "def forwardProp(X, weights):\n",
    "    # Initializing an empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Assigning a name to reuse as inputs\n",
    "    inputs = X\n",
    "    \n",
    "    # For each layer\n",
    "    for w in weights:\n",
    "        # Add bias term to input\n",
    "        inputs = addBiasTerms(inputs)\n",
    "        \n",
    "        # Y = Sigmoid ( X .* W^T )\n",
    "        outputs.append(sigmoid(np.dot(inputs, w.T)))\n",
    "        \n",
    "        # Input of next layer is output of this layer\n",
    "        inputs = outputs[-1]\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "$$ Y^{(l)}_{n{\\times}o_{l}} = Sigmoid\\;(\\;X^{(l)}_{n{\\times}i_{l}} \\; .* \\; W^{(l)}{^{T}}_{i_{l}{\\times}o_{l}} \\;)\\; $$\n",
    "\n",
    "Neural networks are advantageous when we are able to compute that $W$ which satisfies $Y = Sigmoid(X\\cdot*W)$, for given $X$ and $Y$ (in supervised training).\n",
    "\n",
    "But, since there are so many weights (for bigger networks), it is time-intensive to algebraically solve the above equation. (Something like $W = X^{-1} \\;.*\\; Sigmoid^{-1}(Y)$...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set W to minimize cost (computationally intensive)\n",
    "\n",
    "A quicker way to compute W would be to randomly initialize it, and keep updating its value in such a way as to decrease the cost of the neural network.\n",
    "\n",
    "Define the cost as the mean squared error of the output of the neural network:\n",
    "\n",
    "$$error = yPred-\\hat{Y}$$\n",
    "\n",
    "Here, $yPred$ = ``forwardProp``$(X)$, and $\\hat{Y}$ is the desired value of $Y$.\n",
    "\n",
    "$$Cost \\; J = \\frac{1}{2} \\sum \\limits_{n} \\frac{ {\\left( error \\right)}^2 }{n} = \\frac{1}{2} \\sum \\limits_{n} \\frac{ {\\left( yPred-\\hat{Y} \\right)}^2 }{n}$$\n",
    "\n",
    "Once we have initialized W, we need to change it such that J is minimized.\n",
    "\n",
    "The best way to minimize J w.r.t. W, is to partially derive J w.r.t. W and equate it to 0: $\\frac{{\\partial}J}{{\\partial}W} = 0$. But, this is computationally intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly initialize W, change it to decrease cost (more feasible)\n",
    "\n",
    "Instead, we initialize $W$ by randomly sampling from a standard normal distribution, and then keep changing $W$ so as to decrease the cost $J$.\n",
    "\n",
    "But what value to change $W$ by? To find out, let us differentiate $J$ by $W^{(L)}$ (weight matrix of the last layer) and see what we get:\n",
    "\n",
    "$$\\frac{ {\\partial}J} {{\\partial}W^{(L)} }=\\frac{\\partial}{{\\partial}W^{(L)}}\\left(\\frac{1}{2}\\sum\\limits_{n}{\\frac{ {\\left( yPred-\\hat{Y} \\right)}^2 }{n} }\\right)=\\frac{1}{2*n}\\sum\\limits_{n} \\left( \\frac{\\partial} {{\\partial}W^{(L)}} (yPred-\\hat{Y})^2 \\right)=\\frac{1}{n}\\sum\\limits_{n} \\left( (yPred-\\hat{Y}) \\cdot \\frac {{\\partial} \\; yPred} { {\\partial}W^{(L)} } \\right)$$\n",
    "\n",
    "$$\\Rightarrow \\frac{ {\\partial}J} {{\\partial}W^{(L)} } = \\frac{1}{n}\\sum\\limits_{n} \\left( (error) \\cdot \\frac {{\\partial} \\; yPred} { {\\partial}W^{(L)} }  \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating the above equation for numerical analysis:\n",
    "\n",
    "$${\\Delta}J ={{\\Delta}W^{(L)}} * \\left[ \\frac{1}{n}\\sum\\limits_{n} \\left( (error) \\cdot \\frac {{\\partial} \\; yPred} { {\\partial}W^{(L)} } \\right) \\right] \\;\\;\\;\\;\\;\\;-------------(1)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change $W^{(L)}$ so that $J$ always decreases\n",
    "\n",
    "If we ensure that ${{\\Delta}W^{(L)}} = -\\left[ \\frac{1}{n}\\sum\\limits_{n} \\left( (error) \\cdot \\frac {{\\partial} \\; yPred} { {\\partial}W^{(L)} } \\right) \\right]$, we see that ${\\Delta}J = {{\\Delta}W^{(L)}}*\\left(-\\left[{{\\Delta}W^{(L)}}\\right]\\right) = -\\left[{{\\Delta}W^{(L)}}\\right]^{2} \\Rightarrow$ negative! \n",
    "\n",
    "Thus, we change $W$ by that amount which ensures $J$ always decreases!\n",
    "\n",
    "$${{\\Delta}W^{(L)}} = -\\left[ \\frac{1}{n}\\sum\\limits_{n} \\left( (error) \\cdot \\frac {{\\partial} \\; yPred} { {\\partial}W^{(L)} } \\right) \\right] \\;\\;\\;\\;\\;\\;-------------(2)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $W^{(L)}$\n",
    "\n",
    "To compute ${\\Delta}W$, we need to compute $error$ and $\\frac{{\\partial}\\;yPred}{{\\partial}W^{(L)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing error\n",
    "\n",
    "$ error = (yPred) - \\hat{Y} = $ ``forwardProp(X)`` $ - \\hat{Y} \\;\\;\\;\\;\\;\\;-------------(3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose we want to compute those $W$'s in a 3-neuron network that are able to perform AND logic on two inputs.\n",
    "\n",
    "Here, for $X = \\left[\\begin{array}{c}(0,0)\\\\(0,1)\\\\(1,0)\\\\(1,1)\\end{array}\\right]$, $\\hat{Y} = \\left[\\begin{array}{c}0\\\\0\\\\0\\\\1\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:\n",
      "1\n",
      "(2, 3)\n",
      "[[-0.17485632 -2.30228101 -0.48034053]\n",
      " [ 0.84314964  1.07391944  0.58713279]]\n",
      "2\n",
      "(1, 3)\n",
      "[[ 0.86482232  0.51864617  1.07953998]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "layers = [2, 2, 1]\n",
    "weights = initializeWeights(layers)\n",
    "\n",
    "print(\"weights:\")\n",
    "for i in range(len(weights)):\n",
    "    print(i+1); print(weights[i].shape); print(weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our weights have been randomly initialized. Let us see what yPred they give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.86486132]\n",
      " [ 0.87138219]\n",
      " [ 0.86367565]\n",
      " [-0.13142692]]\n"
     ]
    }
   ],
   "source": [
    "# Declare input and desired output for AND gate\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Calculate yPred as the last output from forward propagation\n",
    "yPred = forwardProp(X, weights)[-1]\n",
    "\n",
    "# Error = yPred - Y\n",
    "error = yPred - Y\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
